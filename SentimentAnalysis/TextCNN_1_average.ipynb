{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "限制显存大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(gpus) > 0\n",
    "\n",
    "try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "except :\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "sentiment = []\n",
    "\n",
    "# 载入train\n",
    "file_path = \"../data/aclImdb/train\"\n",
    "for file in os.listdir(os.path.join(file_path, 'neg')):\n",
    "    path = os.path.join(file_path, 'neg', file)\n",
    "    with open(path) as f:\n",
    "        text.append(f.readline())\n",
    "sentiment = [0 for _ in os.listdir(os.path.join(file_path, 'neg'))]\n",
    "\n",
    "for file in os.listdir(os.path.join(file_path, 'pos')):\n",
    "    path = os.path.join(file_path, 'pos', file)\n",
    "    with open(path) as f:\n",
    "        text.append(f.readline())\n",
    "pos_sentiment = [1 for _ in os.listdir(os.path.join(file_path, 'pos'))]\n",
    "sentiment.extend(pos_sentiment)\n",
    "\n",
    "# 载入test\n",
    "text_test = []\n",
    "sentiment_test = []\n",
    "file_path = \"../data/aclImdb/test\"\n",
    "for file in os.listdir(os.path.join(file_path, 'neg')):\n",
    "    path = os.path.join(file_path, 'neg', file)\n",
    "    with open(path) as f:\n",
    "        text_test.append(f.readline())\n",
    "sentiment_test = [0 for _ in os.listdir(os.path.join(file_path, 'neg'))]\n",
    "\n",
    "for file in os.listdir(os.path.join(file_path, 'pos')):\n",
    "    path = os.path.join(file_path, 'pos', file)\n",
    "    with open(path) as f:\n",
    "        text_test.append(f.readline())\n",
    "pos_sentiment_test = [1 for _ in os.listdir(os.path.join(file_path, 'pos'))]\n",
    "sentiment_test.extend(pos_sentiment_test)\n",
    "\n",
    "# 合并train、test数据集\n",
    "text.extend(text_test)\n",
    "sentiment.extend(sentiment_test)\n",
    "print(len(text),len(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    sentence = re.sub('(<)br\\s*\\/*(>)','',sentence) # Removing <br> as coming in reviews due to html pages\n",
    "    sentence = re.sub('([?.!])',r' \\1 ',sentence) # Putting space between word and punctuation\n",
    "    sentence = re.sub('[^a-zA-z!.?]+',' ',sentence) # Replacing everythong other that a-zA-z and some punctuation\n",
    "    sentence = re.sub('(\\.\\s+)+','. ',sentence)\n",
    "    sentence = re.sub('(\\s+[a-z]\\s+)',' ',sentence) # Replacing every single character in between sentence\n",
    "    sentence = sentence.lower() # lowering all words\n",
    "    return sentence\n",
    "\n",
    "processed_text = [preprocess_text(_) for _ in text]\n",
    "print(len(processed_text), len(sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 300\n",
    "vocab_size = 60000\n",
    "BATCH_SIZE = 128\n",
    "EMBEDDING_SIZE = 128\n",
    "EPOCHS = 32\n",
    "learning_rate = 0.0001\n",
    "kernel_l2 = 0.001\n",
    "recurrent_l2 = 0.0005\n",
    "activity_l2 = 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "序列化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(processed_text)\n",
    "\n",
    "processed_text_vector = tokenizer.texts_to_sequences(processed_text)\n",
    "padded_text_vector = tf.keras.preprocessing.sequence.pad_sequences(processed_text_vector,maxlen=max_length, \n",
    "                                                                   truncating='post', padding='post')\n",
    "print(padded_text_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分训练集与测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集，前25000条数据\n",
    "train_text = padded_text_vector[:25000]\n",
    "train_sentiment = sentiment[:25000]\n",
    "train_text, train_sentiment = shuffle(train_text, np.array(train_sentiment), random_state=2)\n",
    "# 测试集，后25000条数据\n",
    "test_text = padded_text_vector[25000:]\n",
    "test_sentiment = sentiment[25000:]\n",
    "test_text, test_sentiment = shuffle(test_text, np.array(test_sentiment), random_state=3)\n",
    "\n",
    "print(len(train_text), len(test_text), len(train_sentiment), len(test_sentiment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "划分训练集与验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, text_val, sentiment_train, sentiment_val = train_test_split(train_text,train_sentiment, test_size=0.2,\n",
    "                                                                       random_state=2)\n",
    "print(len(text_train), len(text_val), len(sentiment_train), len(sentiment_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建TextCNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=EMBEDDING_SIZE, input_length=max_length))\n",
    "model.add(tf.keras.layers.Conv1D(64, 5, activation='relu', kernel_regularizer=regularizers.l2(kernel_l2)))\n",
    "model.add(tf.keras.layers.AveragePooling1D(3))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=text_train,y=sentiment_train, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True,\n",
    "          validation_data=[text_val,sentiment_val], callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制模型训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "subplots_adjust(left=0.0,bottom=0.0,top=3,right=2)\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"train loss\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title(\"train accuracy\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"validation loss\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('acc')\n",
    "plt.title(\"validation accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试集上评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_text, test_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制模型图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file=\"CNN_1_ave.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '''I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?'''\n",
    "x = preprocess_text(x)\n",
    "vec = tokenizer.texts_to_sequences([x])\n",
    "pad_vec = tf.keras.preprocessing.sequence.pad_sequences(vec, maxlen=max_length, truncating='post', padding='post')\n",
    "result = model.predict(pad_vec)\n",
    "print(result > 0.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cd19113c0ce267f95d331de12e62f671b4ae3be9ce70ab9c2c2ed8a73fec64bf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('tf1.15': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}